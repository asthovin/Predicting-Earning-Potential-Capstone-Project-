{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee23df14-210f-4fb3-9dea-a5fc83c60d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#importing important libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Stats & ML\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "   accuracy_score, precision_score, recall_score, f1_score,\n",
    "   classification_report\n",
    ")\n",
    "\n",
    "# Plot settings (if running in notebook, uncomment)\n",
    "# %matplotlib inline\n",
    "sns.set(style=\"whitegrid\", context=\"talk\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "#  LOAD DATA\n",
    "\n",
    "df = pd.read_csv(\n",
    "   'adult.data',   # <-- replace with your file path\n",
    "   header=None,\n",
    "   names=[\n",
    "       \"age\", \"workclass\", \"fnlwgt\", \"education\", \"edyears\",\n",
    "       \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
    "       \"capital-gain\", \"capital-loss\", \"WorkHPweek\", \"country\", \"income\"\n",
    "   ],\n",
    "   na_values=['?'],\n",
    "   skipinitialspace=True\n",
    ")\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "display(df.sample(10))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 2) MISSING VALUES HANDLING\n",
    "\n",
    "print(\"\\nMissing values before dropna():\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "print(\"\\nShape after dropna():\", df.shape)\n",
    "print(\"\\nMissing values after dropna():\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 3) BASIC STRING CLEANING\n",
    "# -------------------------------------------------------\n",
    "strcolumns = df.select_dtypes(include='object').columns\n",
    "for col in strcolumns:\n",
    "   df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "# Normalize 'country' naming variants\n",
    "df[\"country\"] = df[\"country\"].replace({\n",
    "   \"United States\": \"US\",\n",
    "   \"United-States\": \"US\",\n",
    "   \"USA\": \"US\"\n",
    "})\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 4) TYPE CASTING TO NUMERIC (where needed)\n",
    "# -------------------------------------------------------\n",
    "todonum = [\"age\", \"fnlwgt\", \"edyears\", \"capital-gain\", \"capital-loss\", \"WorkHPweek\"]\n",
    "df[todonum] = df[todonum].apply(pd.to_numeric)\n",
    "print(\"\\nDtypes:\\n\", df[todonum].dtypes)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 5) OUTLIER REMOVAL (domain caps + IQR)\n",
    "# -------------------------------------------------------\n",
    "# Domain-based sensible caps\n",
    "df = df[(df[\"age\"] >= 16) & (df[\"age\"] <= 90)]\n",
    "df = df[(df[\"WorkHPweek\"] >= 1) & (df[\"WorkHPweek\"] <= 100)]\n",
    "\n",
    "# IQR-based filtering helper\n",
    "def remove_iqr(dfin, col):\n",
    "   Q1 = dfin[col].quantile(0.25)\n",
    "   Q3 = dfin[col].quantile(0.75)\n",
    "   IQR = Q3 - Q1\n",
    "   low = Q1 - 1.5 * IQR\n",
    "   high = Q3 + 1.5 * IQR\n",
    "   return dfin[(dfin[col] >= low) & (dfin[col] <= high)]\n",
    "\n",
    "for col in [\"fnlwgt\", \"edyears\", \"capital-gain\", \"capital-loss\"]:\n",
    "   df = remove_iqr(df, col)\n",
    "\n",
    "print(\"\\nShape after outlier removal:\", df.shape)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 6) DROP IRRELEVANT/LESS USEFUL COLUMNS FOR THIS ANALYSIS\n",
    "# -------------------------------------------------------\n",
    "cols_to_drop = [\"fnlwgt\", \"relationship\", \"country\", \"capital-gain\", \"capital-loss\"]\n",
    "df = df.drop(columns=cols_to_drop)\n",
    "print(\"\\nDropped columns:\", cols_to_drop)\n",
    "print(\"Shape after dropping columns:\", df.shape)\n",
    "display(df.head())\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 7) CREATE BINARY INCOME TARGET\n",
    "# -------------------------------------------------------\n",
    "df[\"income_bin\"] = df[\"income\"].apply(lambda x: 1 if \">50K\" in x else 0)\n",
    "display(df.head(1))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 8) WEEKLY HOURS vs INCOME (Visualization + t-test)\n",
    "# -------------------------------------------------------\n",
    "plt.figure(figsize=(7, 5))\n",
    "x = df[df.income_bin == 0][\"WorkHPweek\"]\n",
    "y = df[df.income_bin == 1][\"WorkHPweek\"]\n",
    "plt.boxplot([x, y], labels=[\"<=50K\", \">50K\"])\n",
    "plt.title(\"Weekly Working Hours by Income Category\")\n",
    "plt.ylabel(\"Hours per week\")\n",
    "plt.yticks(np.arange(0, 101, 10))\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.4)\n",
    "plt.show()\n",
    "\n",
    "tstat, pval = stats.ttest_ind(\n",
    "   df[df.income_bin == 1][\"WorkHPweek\"],\n",
    "   df[df.income_bin == 0][\"WorkHPweek\"],\n",
    "   equal_var=False\n",
    ")\n",
    "print(\"\\nT-test (Weekly Hours vs Income):\")\n",
    "print(\"t-statistic:\", round(tstat, 4), \"| p-value:\", pval)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 9) CORRELATION HEATMAP (numeric features)\n",
    "# -------------------------------------------------------\n",
    "num_for_corr = df[[\"age\", \"edyears\", \"WorkHPweek\", \"income_bin\"]]\n",
    "corr = num_for_corr.corr(method=\"pearson\")\n",
    "print(\"\\nCorrelation matrix (numeric features):\\n\", corr)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(\n",
    "   corr, annot=True, cmap=\"coolwarm\",\n",
    "   vmin=-1, vmax=1, linewidths=0.5, square=False\n",
    ")\n",
    "plt.title(\"Correlation Heatmap (Pearson)\")\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 10) EDUCATION vs INCOME\n",
    "# -------------------------------------------------------\n",
    "edu = df.groupby(\"edyears\")[\"income_bin\"].mean().reset_index()\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(edu[\"edyears\"], edu[\"income_bin\"], marker=\"o\")\n",
    "plt.xlabel(\"Education (years)\")\n",
    "plt.ylabel(\"Probability of earning >50K\")\n",
    "plt.title(\"Education vs Earning Potential\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "edu_corr = df['edyears'].corr(df['income_bin'])\n",
    "print(\"\\nCorrelation (Education years vs Income_bin):\", round(edu_corr, 4))\n",
    "\n",
    "high_edu = df[df['edyears'] >= df['edyears'].median()]['income_bin']\n",
    "low_edu = df[df['edyears'] < df['edyears'].median()]['income_bin']\n",
    "tstat, pval = stats.ttest_ind(high_edu, low_edu, equal_var=False)\n",
    "print(\"T-test (High vs Low Education): t-stat =\", round(tstat, 4), \", p-value =\", pval)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 11) AGE vs INCOME (scatter)\n",
    "# -------------------------------------------------------\n",
    "plt.figure(figsize=(7, 5))\n",
    "jitter = (np.random.rand(len(df)) - 0.5) / 10  # small jitter to visualize 0/1 spread\n",
    "plt.scatter(df[\"age\"], df[\"income_bin\"] + jitter, alpha=0.25, s=10)\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Income (0 or 1)\")\n",
    "plt.title(\"Age vs Earning Potential\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 12) *** NEW BLOCK *** CATEGORICAL vs INCOME ASSOCIATION\n",
    "#     - Chi-square test of independence\n",
    "#     - Cramér's V (strength of association)\n",
    "#     - Bar charts of P(>50K) by category\n",
    "# -------------------------------------------------------\n",
    "\n",
    "# Select categorical columns (strings) excluding target 'income'\n",
    "cat_cols = df.select_dtypes(include=\"object\").columns.tolist()\n",
    "cat_cols = [c for c in cat_cols if c != \"income\"]\n",
    "\n",
    "assoc_results = []\n",
    "\n",
    "def cramers_v_from_crosstab(ct):\n",
    "   \"\"\"\n",
    "   Compute Cramér's V given a contingency table.\n",
    "   V = sqrt(chi2 / (n * (min(k-1, r-1))))\n",
    "   \"\"\"\n",
    "   chi2, p, dof, expected = chi2_contingency(ct)\n",
    "   n = ct.values.sum()\n",
    "   r, k = ct.shape\n",
    "   denom = n * (min(r - 1, k - 1))\n",
    "   if denom == 0:\n",
    "       return np.nan, p, chi2\n",
    "   V = np.sqrt(chi2 / denom)\n",
    "   return V, p, chi2\n",
    "\n",
    "print(\"\\n=== Categorical Features: Chi-square & Cramér's V vs Income ===\")\n",
    "for c in cat_cols:\n",
    "   # Build contingency table between category and income_bin\n",
    "   ct = pd.crosstab(df[c], df[\"income_bin\"])\n",
    "   V, p, chi2 = cramers_v_from_crosstab(ct)\n",
    "   assoc_results.append({\"feature\": c, \"cramers_v\": V, \"p_value\": p, \"chi2\": chi2})\n",
    "\n",
    "   # Print quick summary\n",
    "   print(f\"{c:15s} | Cramér's V: {V:.4f} | p-value: {p:.3e} | chi2: {chi2:.2f} | levels: {ct.shape[0]}\")\n",
    "\n",
    "# Summarize associations sorted by strength\n",
    "assoc_df = pd.DataFrame(assoc_results).sort_values(by=\"cramers_v\", ascending=False)\n",
    "print(\"\\n\\nCategorical–Income association (sorted by Cramér's V):\")\n",
    "display(assoc_df)\n",
    "\n",
    "# Bar charts: Probability of >50K by category (sorted)\n",
    "for c in cat_cols:\n",
    "   prob = df.groupby(c)[\"income_bin\"].mean().sort_values(ascending=False).reset_index()\n",
    "   plt.figure(figsize=(10, 4 + 0.15 * prob.shape[0]))  # dynamic height if many categories\n",
    "   sns.barplot(data=prob, x=\"income_bin\", y=c, orient=\"h\", palette=\"viridis\")\n",
    "   plt.xlabel(\"P(Income > 50K)\")\n",
    "   plt.ylabel(c)\n",
    "   plt.title(f\"P(>50K) by {c} (sorted)\")\n",
    "   plt.xlim(0, 1)\n",
    "   plt.tight_layout()\n",
    "   plt.show()\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 13) MODELING: Label Encode categorical columns for Logistic Regression\n",
    "# -------------------------------------------------------\n",
    "le = LabelEncoder()\n",
    "for c in cat_cols:\n",
    "   df[c] = le.fit_transform(df[c])  # convert str categories to numeric labels\n",
    "\n",
    "print(\"\\nCategorical columns encoded for modeling.\")\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 14) TRAIN/TEST SPLIT & LOGISTIC REGRESSION\n",
    "# -------------------------------------------------------\n",
    "X = df.drop(columns=[\"income\", \"income_bin\"])\n",
    "y = df[\"income_bin\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "   X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "model = LogisticRegression(max_iter=300)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"\\n=== MODEL PERFORMANCE ===\")\n",
    "print(\"Accuracy :\", round(accuracy_score(y_test, y_pred), 4))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred), 4))\n",
    "print(\"Recall   :\", round(recall_score(y_test, y_pred), 4))\n",
    "print(\"F1 Score :\", round(f1_score(y_test, y_pred), 4))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# 15) FEATURE IMPORTANCE (Logistic Regression Coefficients)\n",
    "# -------------------------------------------------------\n",
    "feature_importance = pd.DataFrame({\n",
    "   'Feature': X.columns,\n",
    "   'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance (Logistic Regression Coefficients):\")\n",
    "display(feature_importance)\n",
    "\n",
    "# Highlight Weekly Working Hours coefficient\n",
    "wh_coef = feature_importance.loc[feature_importance['Feature'] == 'WorkHPweek', 'Coefficient']\n",
    "if not wh_coef.empty:\n",
    "   print(\"\\nWeekly Working Hours Coefficient:\", float(wh_coef.values[0]))\n",
    "\n",
    "# Visualize coefficients\n",
    "plt.figure(figsize=(8, max(6, 0.35 * len(feature_importance))))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Coefficient'], color='steelblue')\n",
    "plt.title(\"Feature Importance (Logistic Regression Coefficients)\")\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (profiling_env)",
   "language": "python",
   "name": "python313"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
